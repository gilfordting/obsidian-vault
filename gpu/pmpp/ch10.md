reduction
- array of values -> single value
- sum, max val, min val, etc
# background
- reduction can be for any binary operator if any well-defined identity
# reduction trees
- order of performing ops will be changed
- this assumes that order of applying the operator does not matter
- we need an associative operator
- subtraction is not associative!
- we need to be able to insert arbitrary parentheses
- floating-point adds not actually associative, but it's tolerable results
- so in effect we are just inserting different parentheses
- optimizations will also rearrange order of operands
- reduction not to be confused with tree data structures
- here, the edges are conceptual, information flow
- we don't get lower latency for free
- need hardware comparators enough to perform up to four max ops
- total number of ops is a geometric series, summing to same as sequential algorithm
- we can make things logarithmic but we need at least $N/2$ exec resources
- parallel reduction is like a bracket, in a competition
- sharing resources makes it more viable, but also leads to slowdown again
# simple reduction kernel
- sum within single block
- fewer and fewer threads are active as we go along
- `__syncthreads()` for read-after-write dependency
# minimizing control divergence
- we get lots of it
- only the threads whose `threadIdx.x` values even will execute addition -- lose half
- also, if spread across multiple warps, then some threads are just useless for some iterations
- we can assign threads to input array locations better
- the distance between active threads increases
- instead, let's arrange threads so they remain close
- stride should *decrease*, not increase!
- but this means we reorder operands, not insert parentheses
- why is there a difference in control divergence?
- position of threads that perform addition op, relative to those that don't
- what happens?
	- first iteration: all threads active
	- second iteration: 0-63 active, 64-127 inactive
		- warps 2-3 *all* inactive, so no control divergence
- we can't avoid all divergence
- the number of threads executing add will fall below 32
# minimizing memory divergence
- important to achieve memory coalescing within warps
- but adjacent threads don't access adjacent locations
- we access two locations to read: owned location, and the stride distance away
- write is to owned location
- locations owned by adjacent thread are not adjacent locations
- in 10.7:
	- all threads in warp perform first read: locations two elements away from each other
	- half of the data returned by global mem not used by threads
	- same deal for second read, and the write
	- also an issue for further iterations
- in 10.9:
	- adjacent threads in each warp access adjacent locations in global mem
- so our convergent kernel offers more efficiency in using execution resources, and dram bandwidth
# minimizing global memory accesses
- we can write partial sum result values to *shared* memory!
# hierarchical reduction for arbitrary input length
- so far, we assume one thread block
- because `__syncthreads()` is used, which is only for threads in same block
- parallelism is limited to 1024 threads on current hardware
- we can do hierarchical, segmented multiblock reduction
- partition input array into segments, so each segment is appropriate size for block
- then each block does a reduction tree
- accumulation is done with atomic add
- each block processes `2 * blockDim.x` elements
- once we know starting location, threads can just work on it as if it's the entire input data
# thread coarsening for reduced overhead
- for reduction of $n$ elements, $n/2$ thread launched
- thread block size of 1024 threads? # thread blocks is $N/2048$
- what if the hardware can only execute a portion of the thread blocks in parallel?
- we've paid a price to distribute work across thread blocks
- hardware underutilization increases with each stage of reduction tree
- thread coarsening serializes into fewer threads; avoids parallelization overhead
- add four elements before reduction tree
- we can keep coarsening, but we'll lose data parallelism
- we need enough thread blocks to fully utilize hardware
- 