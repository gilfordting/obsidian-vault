prefix sum (scan)
- used to parallelize seemingly sequential operations
	- resource alloc, work assignment, polynomial eval
- if computation can be described as recursion in which each item is defined in terms of previous item, can be parallelized
- plays a key role in massively parallel computing, because any sequential section can limit the overall perf
- lots of seq. sections can be converted into parallel comp. with parallel scan
# background
- inclusive scan takes binary associative op $\oplus$ and input array $[x_0, x_1, \dots, x_{n-1}]$ and returns output:
	- $[x_0, (x_0 \oplus x_1), \dots, (x_0 \oplus x_1 \oplus \dots \oplus x_{n-1})]$
- inclusive scan because each output includes effect of corresponding input
- one way to think about it: take request, and identify all the separating points to serve all orders at once in parallel
- or, each output excludes effect of corresponding input element
- prefix sum minus original array, element-wise; first element is identity
- can easily convert via shifting; just depends if we care about cut points or beginning points
- so we'll just do inclusive scan
- assume input in `x`, output in `y`
- main loop body is `y[i] = y[i-1] + x[i]`
- we'll do a parallel segmented scan
	- thread block performs scan on segment
	- then we'll combine the segmented scan results into output for entire input arr
# parallel scan with kogge-stone
- perform a reduction op for each output element
- what if we use each thread to perform sequential reduction for one output?
- doesn't do any better, calculation of $y_{n-1}$ takes $n$ steps
- in fact we are repeating a lot of work with no speedup
- parallel reduction tree is better
- we need to share partial sums across reduction trees of diff output elements
- let's use the kogge-stone algorithm
- array `XY` that originally contains input elements; iteratively evolves contents of array into outputs
- before algorithm begins, `XY[i]` contains $x_i$
- after $k$ iters, `XY[i]` contains sum of up to $2^k$ input elements at and before location
	- e.g. at the end of iteration 2, `XY[i]` has $x_{i-3} + x_{i-2}+x_{i-1}+x_i$
- 16-element example
	- first iteration: every position except `XY[0]` gets sum of current and left neighbor
	- `XY[i]` has $x_{i-1}+x_i$
	- second iteration: every pos except `XY[0]` and `XY[1]` gets sum of current and position 2 elements away
- cuda
	- we need syncthreads after reading and before write, because we use the same array
	- write-after-read data hazard
	- previous race condition as in parallel histogram was read-modify-write race condition that can be solved with atomics
	- we can also use double buffering here
	- there's also control divergence
		- but adjacent threads tend to execute same number of iterations?
# speed and work efficiency considerations
- work efficiency?
	- the extent to which the work performed is close to minimum amount needed
- min number for scan is $N-1$ ops, $O(N)$
- naive parallel uses $N(N-1)/2$ ops -- not work efficient
- all threads iterate up to $\log_2(N)$ steps, $N$ is `SECTION_SIZE`
- total work done is $N\log_2 N - (N-1)$
- still not as work efficient as sequential
- but it uses fewer steps
- for-loop is $N$ iters, reduction is $\log_2 N$
- not as good b/c less efficient hardware usage, and more energy consumption
- but can get good speed if enough hardware resources
- newer GPU arch generations mean shuffle instructions can be used
# parallel scan with brent-kung
- there is a reverse tree used to distribute partial sums
	- first, odd positions are completed
- accumulate sum value towards highest position
	- reach for partial sum to left
	- significant control divergence
	- so we can get a more sophisticated thread index to data index mapping -- continuous section of threads maps to series of data positions that are stride distance apart
	- control div doesn't happen until # active threads drops below warp size
- brent-kung < koffe-stone because potentially longer exec time, despite higher work efficiency
- infinite exec resources? brent-kung about twice as long as kogge-stone
- additional steps needed for reverse tree phase
- if limited exec resources, control divergence matters
# coarsening for even more work efficiency
- each thread works on blocks of 4
- if perform scan by accessing input from global mem, accesses not coalesced
- so transfer gmem -> smem in colesced manner, then unfavorable access pattern in smem
# segmented parallel scan for arbitrary-length inputs
- hierarchical scan approach
- scan blocks
- need scan on output elements -- these are the last result elements from each scan block
- these are also similar to carry look-ahead adders
# single-pass scan for mem access efficiency
- in segmented scan, partially scanned results (scan blocks) stored in gmem before launching global scan kernel
- reloaded back from global mem by third kernel
- time for perfoming extra mem stores/loads not overlapped with computation in subsequent kernels
- stream-based or domino-style is segmented scan alg in which partial sum data passed in one direction through global memory between adjacent thread blocks
- thread block $i$ performs scan on its scan block, then wait for left neighbor block $i-1$ to pass sum value, pass along sum, and move on to add sum value to all outputs
- serialized during data-passing phase, but afterwards some overlap
- set and check flags
- leader thread per-block
- need `__threadfence()` to ensure scan value arrives to gmem before flag set with atomicAdd
- the flags might incur global mem traffic? no, L2 cache
- one subtle issue: thread blocks not always scheduled in order according to `blockIdx` -- this can lead to deadlock
- can use global counter to do dynamic block index assignment
- 